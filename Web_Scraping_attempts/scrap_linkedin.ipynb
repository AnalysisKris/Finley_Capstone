{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract job details\n",
    "def extract_job_details(job_soup):\n",
    "    job_post = {}\n",
    "\n",
    "    try:\n",
    "        job_post[\"job_title\"] = job_soup.find(\"h2\", {\"class\": \"top-card-layout__title\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"job_title\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"company_name\"] = job_soup.find(\"a\", {\"class\": \"topcard__org-name-link\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"company_name\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"time_posted\"] = job_soup.find(\"span\", {\"class\": \"posted-time-ago__text\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"time_posted\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"num_applicants\"] = job_soup.find(\"span\", {\"class\": \"num-applicants__caption\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"num_applicants\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"job_description\"] = job_soup.find(\"div\", {\"class\": \"description__text description__text--rich\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"job_description\"] = None\n",
    "\n",
    "    return job_post\n",
    "\n",
    "\n",
    "# Example variables\n",
    "title = \"Data Scientist\"\n",
    "location = \"Missouri\"\n",
    "start = 0\n",
    "\n",
    "# Construct the URL for LinkedIn job search\n",
    "list_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={title}&location={location}&start={start}\"\n",
    "\n",
    "# Send a GET request to the URL and store the response\n",
    "response = requests.get(list_url)\n",
    "\n",
    "# Get the HTML, parse the response and find all list items (job postings)\n",
    "list_data = response.text\n",
    "list_soup = BeautifulSoup(list_data, \"html.parser\")\n",
    "page_jobs = list_soup.find_all(\"li\")\n",
    "\n",
    "# Create an empty list to store the job IDs\n",
    "id_list = []  \n",
    "\n",
    "# Iterate through job postings to find job IDs\n",
    "for job in page_jobs:\n",
    "    base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "    job_id = base_card_div.get(\"data-entity-urn\").split(\":\")[3]\n",
    "    id_list.append(job_id)\n",
    "\n",
    "# Initialize an empty list to store job information\n",
    "job_list = []\n",
    "\n",
    "# Loop through the list of job IDs and get each URL\n",
    "for job_id in id_list:\n",
    "    # Construct the URL for each job using the job ID\n",
    "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "\n",
    "    # Send a GET request to the job URL and parse the response\n",
    "    job_response = requests.get(job_url)\n",
    "    job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "\n",
    "    # Extract job details\n",
    "    job_post = extract_job_details(job_soup)\n",
    "\n",
    "    # Append the job details to the job_list\n",
    "    job_list.append(job_post)\n",
    "\n",
    "# Create a pandas DataFrame using the list of job dictionaries 'job_list'\n",
    "jobs_df = pd.DataFrame(job_list)\n",
    "\n",
    "# Save data to CSV file\n",
    "jobs_df.to_csv('MO_DS_Linkedin.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to pickle file\n",
    "with open('US_DS_Linkedin.pkl', 'wb') as f:\n",
    "    pickle.dump(job_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       job_title       company_name  \\\n",
      "0                                 Data Scientist                Glo   \n",
      "1                        Data Scientist, Product             Notion   \n",
      "2  Data Scientist (L5) - Content & Marketing DSE            Netflix   \n",
      "3                                 Data Scientist    Rue Gilt Groupe   \n",
      "4                                 Data Scientist    Rue Gilt Groupe   \n",
      "5                                 Data Scientist   Carrot Fertility   \n",
      "6      Staff Data Scientist, Strategy & Insights           LinkedIn   \n",
      "7              Data Scientist, Product Analytics               Etsy   \n",
      "8                          Junior Data Scientist  Team Remotely Inc   \n",
      "\n",
      "    time_posted  num_applicants  \\\n",
      "0  14 hours ago  174 applicants   \n",
      "1    4 days ago            None   \n",
      "2    4 days ago            None   \n",
      "3    5 days ago            None   \n",
      "4    1 week ago            None   \n",
      "5  21 hours ago            None   \n",
      "6    5 days ago  197 applicants   \n",
      "7    1 week ago  176 applicants   \n",
      "8  12 hours ago   41 applicants   \n",
      "\n",
      "                                     job_description  \n",
      "0  Position Overview:The Data Scientist will form...  \n",
      "1  About UsWe're on a mission to make it possible...  \n",
      "2  It's an exciting time to join Netflix as we co...  \n",
      "3  THE ROLE: Data ScientistJob Location: USA Remo...  \n",
      "4  THE ROLE: Data ScientistJob Location: USA Remo...  \n",
      "5  About CarrotCarrot Fertility is the leading gl...  \n",
      "6  LinkedIn is the worldâ€™s largest professional n...  \n",
      "7  Company DescriptionEtsy is the global marketpl...  \n",
      "8  This is a remote position.Junior Data Scientis...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Function to extract job details\n",
    "def extract_job_details(job_soup):\n",
    "    job_post = {}\n",
    "\n",
    "    try:\n",
    "        job_post[\"job_title\"] = job_soup.find(\"h2\", {\"class\": \"top-card-layout__title\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"job_title\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"company_name\"] = job_soup.find(\"a\", {\"class\": \"topcard__org-name-link\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"company_name\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"time_posted\"] = job_soup.find(\"span\", {\"class\": \"posted-time-ago__text\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"time_posted\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"num_applicants\"] = job_soup.find(\"span\", {\"class\": \"num-applicants__caption\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"num_applicants\"] = None\n",
    "\n",
    "    try:\n",
    "        job_post[\"job_description\"] = job_soup.find(\"div\", {\"class\": \"description__text description__text--rich\"}).text.strip()\n",
    "    except:\n",
    "        job_post[\"job_description\"] = None\n",
    "\n",
    "    return job_post\n",
    "\n",
    "# Example variables\n",
    "title = \"Data Scientist\"\n",
    "location = \"\"\n",
    "start = 0\n",
    "\n",
    "# Construct the URL for LinkedIn job search\n",
    "list_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={title}&location={location}&start={start}\"\n",
    "\n",
    "# Send a GET request to the URL and store the response\n",
    "response = requests.get(list_url)\n",
    "\n",
    "# Get the HTML, parse the response and find all list items (job postings)\n",
    "list_data = response.text\n",
    "list_soup = BeautifulSoup(list_data, \"html.parser\")\n",
    "page_jobs = list_soup.find_all(\"li\")\n",
    "\n",
    "# Create an empty list to store the job IDs\n",
    "id_list = []\n",
    "\n",
    "# Iterate through job postings to find job IDs\n",
    "for job in page_jobs:\n",
    "    base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "    job_id = base_card_div.get(\"data-entity-urn\").split(\":\")[3]\n",
    "    id_list.append(job_id)\n",
    "\n",
    "# Initialize an empty list to store job information\n",
    "job_list = []\n",
    "\n",
    "# Loop through the list of job IDs and get each URL\n",
    "for job_id in id_list:\n",
    "    # Construct the URL for each job using the job ID\n",
    "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "\n",
    "    # Send a GET request to the job URL and parse the response\n",
    "    job_response = requests.get(job_url)\n",
    "    job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "\n",
    "    # Extract job details\n",
    "    job_post = extract_job_details(job_soup)\n",
    "\n",
    "    # Append the job details to the job_list\n",
    "    job_list.append(job_post)\n",
    "\n",
    "# Save data to pickle file\n",
    "with open('DS_Linkedin.pkl', 'wb') as f:\n",
    "    pickle.dump(job_list, f)\n",
    "\n",
    "# Load data from pickle file\n",
    "with open('DS_Linkedin.pkl', 'rb') as f:\n",
    "    job_list_loaded = pickle.load(f)\n",
    "\n",
    "# Create a pandas DataFrame using the loaded job list\n",
    "jobs_df = pd.DataFrame(job_list_loaded)\n",
    "\n",
    "# Display or further process the jobs_df DataFrame as needed\n",
    "print(jobs_df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use BeautifulSoup to parse the HTML content\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m job_soup \u001b[39m=\u001b[39m BeautifulSoup(job_response\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Find the article content\u001b[39;00m\n\u001b[1;32m      5\u001b[0m content \u001b[39m=\u001b[39m job_soup\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mjob_description\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "# Use BeautifulSoup to parse the HTML content\n",
    "job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "    \n",
    "# Find the article content\n",
    "content = job_soup.find(\"job_description\")\n",
    "\n",
    "# Serialize the extracted article HTML to a .pkl file\n",
    "with open('DS_Linkedin.pkl', 'wb') as file:\n",
    "    pickle.dump(str(content), file)\n",
    "\n",
    "print(\"LinkedIn's HTML content has been saved to 'HTML_content.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b69611a5e908fff52739484b03ed04ac3b781fc2c02522f7e97d7b5506d2f83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
