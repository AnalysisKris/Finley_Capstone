{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Pre-processing, and Feature Engineering\n",
    "## Detecting Gender Bias in Job Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Description Analysis\n",
    "\n",
    "This notebook performs data cleaning and feature extraction on job descriptions, focusing on detecting gender bias through communal and agentic language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pycountry\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from textstat import textstat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK Data\n",
    "\n",
    "Download necessary NLTK data for tokenization, stopwords, and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Data\n",
    "\n",
    "Load the CSV file into a Pandas DataFrame and inspect the first few rows and general information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('job_title_des.csv')\n",
    "\n",
    "# Inspect the data\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "Define functions for cleaning text, lemmatizing, detecting communal and agentic words, extracting seniority levels, and extracting salary information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Testing removal\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Lemmatize text function\n",
    "#def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Define communal and agentic words\n",
    "communal_words = [\n",
    "    'empathy', 'supportive', 'collaborative', 'kind', 'warm', 'compassionate', \n",
    "    'nurturing', 'cooperative', 'helpful', 'understanding', 'friendly', 'patient', \n",
    "    'approachable', 'loyal', 'trustworthy', 'caring', 'empathetic', 'sympathetic', \n",
    "    'considerate', 'tolerant', 'generous', 'amicable', 'benevolent', 'cordial', \n",
    "    'gentle', 'forgiving', 'inclusive', 'reliable', 'sympathizing', 'responsive', \n",
    "    'supporting', 'altruistic', 'dedicated', 'personable', 'sociable', 'neighborly', \n",
    "    'thoughtful', 'cohesive', 'agreeable', 'emotional intelligence', 'community-oriented',\n",
    "    'kind-hearted', 'empathic', 'charitable', 'helping', 'humane', 'cross-functional',\n",
    "    'teamwork', 'collaborate', 'partner', 'team', 'teamwork', 'community', 'share',\n",
    "    'support'\n",
    "]\n",
    "\n",
    "agentic_words = [\n",
    "    'ambition', 'independence', 'assertiveness', 'confident', 'competitive', \n",
    "    'leadership', 'proactive', 'self-reliant', 'dominant', 'persistent', 'decisive', \n",
    "    'driven', 'strategic', 'innovative', 'analytical', 'resilient', 'determined', \n",
    "    'goal-oriented', 'ambitious', 'tenacious', 'self-confident', 'entrepreneurial', \n",
    "    'resourceful', 'problem-solving', 'visionary', 'dynamic', 'risk-taking', \n",
    "    'decisiveness', 'assertive', 'result-oriented', 'influential', 'high-achieving', \n",
    "    'challenger', 'independent thinking', 'self-starter', 'self-assured', 'decisive', \n",
    "    'executive', 'autonomous', 'enterprising', 'bold', 'driving', 'impactful', \n",
    "    'asserting', 'purposeful', 'assertive', 'motivated', 'authority', 'control', 'dominant', \n",
    "    'leadership', 'mastery', 'strong', 'expertise', 'leadership', 'disciplined', 'authority',\n",
    "    'control', 'dominance', 'mastery', 'governance', 'command', 'ascendancy', 'command',\n",
    "    'vigor', 'supremacy', 'rule'    \n",
    "]\n",
    "\n",
    "# Count words function\n",
    "def count_words(tokens, word_list):\n",
    "    return sum(1 for token in tokens if token in word_list)\n",
    "\n",
    "# Detect agentic words function\n",
    "def detect_agentic_words(description):\n",
    "    words = description.split()\n",
    "    detected_words = [word for word in words if word.lower() in agentic_words]\n",
    "    return ' '.join(detected_words)\n",
    "\n",
    "\n",
    "# Detect communal words function\n",
    "def detect_communal_words(description):\n",
    "    words = description.split()\n",
    "    detected_words = [word for word in words if word.lower() in communal_words]\n",
    "    return ' '.join(detected_words)\n",
    "\n",
    "# Extract seniority level from job titles\n",
    "def extract_seniority(title):\n",
    "    title = title.lower()\n",
    "    if any(keyword in title for keyword in ['senior', 'lead', 'manager', 'director']):\n",
    "        return 'senior'\n",
    "    elif any(keyword in title for keyword in ['junior', 'assistant', 'entry']):\n",
    "        return 'junior'\n",
    "    else:\n",
    "        return 'mid'\n",
    "\n",
    "# Extract salary range from job descriptions\n",
    "def extract_salary(description):\n",
    "    salary_pattern = re.compile(r'\\$\\d{2,3}(?:,\\d{3})*(?:-\\$\\d{2,3}(?:,\\d{3})*)?')\n",
    "    match = salary_pattern.search(description)\n",
    "    if match:\n",
    "        salary_str = match.group()\n",
    "        if '-' in salary_str:\n",
    "            lower, upper = salary_str.split('-')\n",
    "            return (int(lower.replace('$', '').replace(',', '')) + int(upper.replace('$', '').replace(',', ''))) / 2\n",
    "        else:\n",
    "            return int(salary_str.replace('$', '').replace(',', ''))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "\n",
    "Apply the helper functions to extract seniority levels, clean job descriptions, detect agentic and communal words, add sentiment scores, and readability scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract seniority level from job titles\n",
    "df['seniority_level'] = df['Job Title'].apply(extract_seniority)\n",
    "\n",
    "# Extract salary information\n",
    "df['salary'] = df['Job Description'].apply(lambda x: extract_salary(x) if isinstance(x, str) else None)\n",
    "\n",
    "# Clean job descriptions\n",
    "df['cleaned_description'] = df['Job Description'].apply(clean_text)\n",
    "df['cleaned_description'] = df['cleaned_description'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Add text length features\n",
    "df['word_count'] = df['cleaned_description'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['cleaned_description'].apply(lambda x: len(x))\n",
    "df['avg_word_length'] = df['cleaned_description'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "df['sentence_count'] = df['cleaned_description'].apply(lambda x: len(TextBlob(x).sentences))\n",
    "df['avg_sentence_length'] = df['word_count'] / df['sentence_count']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Additional Features\n",
    "\n",
    "Detect agentic and communal words, add sentiment scores, and readability scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for detected agentic and communal words\n",
    "df['agentic_words'] = df['cleaned_description'].apply(detect_agentic_words)\n",
    "df['communal_words'] = df['cleaned_description'].apply(detect_communal_words)\n",
    "\n",
    "# Add sentiment score\n",
    "df['sentiment'] = df['cleaned_description'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Add readability scores\n",
    "df['flesch_reading_ease'] = df['cleaned_description'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['flesch_kincaid_grade'] = df['cleaned_description'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.cleaned_description.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Communal and Agentic Scores\n",
    "\n",
    "Calculate communal and agentic scores, as well as their frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate communal and agentic scores\n",
    "df['communal_score'] = df['cleaned_description'].apply(lambda x: count_words(x, communal_words))\n",
    "df['agentic_score'] = df['cleaned_description'].apply(lambda x: count_words(x, agentic_words))\n",
    "\n",
    "# Calculate frequency of communal and agentic words\n",
    "df['communal_freq'] = df['communal_score'] / df['cleaned_description'].apply(len)\n",
    "df['agentic_freq'] = df['agentic_score'] / df['cleaned_description'].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Salary Values\n",
    "\n",
    "Handle missing salary values by imputing with the median salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing salary values if necessary (e.g., imputation, dropping, etc.)\n",
    "df['salary'] = df['salary'].fillna(df['salary'].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "\n",
    "Save the cleaned data with features to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data with features to the current working directory\n",
    "df.to_csv('cleaned_dataset.csv', index=False)\n",
    "print(\"Data cleaning and feature extraction complete. Cleaned data saved to 'cleaned_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Create visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['communal_score'].plot(kind='hist', alpha=0.5, label='Communal Score', color='blue')\n",
    "df['agentic_score'].plot(kind='hist', alpha=0.5, label='Agentic Score', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.title('Distribution of Communal and Agentic Scores in Job Descriptions')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['communal_freq'].plot(kind='hist', alpha=0.5, label='Communal Frequency', color='blue')\n",
    "df['agentic_freq'].plot(kind='hist', alpha=0.5, label='Agentic Frequency', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Distribution of Communal and Agentic Frequencies in Job Descriptions')\n",
    "plt.show()\n",
    "\n",
    "print(df.info())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b69611a5e908fff52739484b03ed04ac3b781fc2c02522f7e97d7b5506d2f83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
